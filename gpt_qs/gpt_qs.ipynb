{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "Question 1: ETL Process\n",
    "Explain the ETL process in data engineering. What are the key steps involved, and why is it important in data integration?"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The ETL process (Extract, Transform, Load) involves extracting data from various sources, transforming it into a usable format, and loading it into a target database or data warehouse. It's crucial for data integration as it ensures data consistency, quality, scalability, and enables data-driven decision-making."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Question 2: SQL Query\n",
    "Write an SQL query to calculate the total revenue generated by a company from a sales table. The table contains columns: sales_id, product_name, quantity, and unit_price."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "SELECT SUM(quantity * unit_price) AS total_revenue FROM sales;"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Question 3: Data Modeling\n",
    "Explain the difference between a star schema and a snowflake schema. When would you use each of them?"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Star Schema:\n",
    "\n",
    "Central fact table surrounded by denormalized dimension tables.\n",
    "Simple and efficient for query performance.\n",
    "Suitable for read-intensive reporting and analytics.\n",
    "Snowflake Schema:\n",
    "\n",
    "Central fact table surrounded by normalized dimension tables.\n",
    "More complex queries but better data integrity and storage efficiency.\n",
    "Suitable for large data warehouses with complex relationships and frequent dimension data updates."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Question 4: Data Pipeline Design\n",
    "You have to design a data pipeline that extracts data from a CSV file, performs some data transformations, and then loads it into a MySQL database. Outline the key components and technologies you would use for this pipeline."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Data Source: CSV File\n",
    "Data Extraction: CSV Parser\n",
    "Data Transformation: ETL Tool\n",
    "Data Storage: MySQL Database\n",
    "Data Loading: SQL Statements\n",
    "Monitoring: Logging and Monitoring Tools"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Question 5: Data Quality\n",
    "How do you ensure data quality in a data pipeline? What are some common data quality issues you might encounter, and how would you handle them?"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Data Profiling: Understand data structure and patterns.\n",
    "Data Validation: Implement checks to verify integrity and consistency.\n",
    "Error Handling: Gracefully manage and log errors.\n",
    "Data Cleansing: Remove duplicates, correct errors, and handle missing values.\n",
    "Data Integration: Resolve conflicts and ensure consistency.\n",
    "Data Quality Metrics: Define and measure accuracy, completeness, consistency, and timeliness."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Question 6: Big Data Technologies\n",
    "What is Apache Spark, and how does it relate to data engineering? How would you use Spark for large-scale data processing?"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Apache Spark is an open-source, distributed computing system used for large-scale data processing. It relates to data engineering as it enables efficient data manipulation, transformation, and integration. To use Spark for large-scale data processing, set up a cluster, ingest data from various sources, apply transformations using Spark's APIs, take advantage of in-memory processing, ensure fault tolerance, utilize parallel processing, and optimize performance. Spark is a valuable tool for data engineers to handle big data and build scalable data processing pipelines."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}